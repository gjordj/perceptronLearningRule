-pf$w[1]/pf$w[2]
library(ggplot2)
DistanceFromPlane = function(z, w, b) {
sum(z * w) + b
}
ClassifyLinear = function(x, w, b) {
distances = apply(x, 1, DistanceFromPlane, w, b)
return(ifelse(distances < 0, -1, +1))
}
EuclideanNorm <- function(x) {
return(sqrt(sum(x * x)))
}
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
# very easy
# x2 = x1 + 1/2
set.seed(1234)
x1 <- runif(100,-1,1)
x2 <- runif(100,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
PlotData <- function(x, y) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(0.5,1)
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PlotData(x, y)
the_perceptron <- PerceptronFunction(x,y)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(sum(abs(y - predicted_y)))
# Trying other learning rates.
#############
lr = 0.00001#
#############
start_time <- Sys.time()
the_perceptron <- PerceptronFunction(x,y,learning.rate = lr)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(sum(abs(y - predicted_y)))
end_time <- Sys.time()
# end_time - start_time
# Time difference of 4.836457 secs  with a learning.rate = 0.5
# Time difference of 4.883949 secs  with a learning.rate = 0.89.
# Time difference of 4.891064 secs  with a learning.rate = 0.99999
# Time difference of 4.894685 secs with a learning.rate = 0.00001
# Time difference of 4.941519 secs with a learning.rate = 0.0001
# Time difference of 5.081375 secs with a  learning.rate = 1.
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
cost_function <- c(cost_function,sum(y != yc))
}
}
}
cost_function <- c(cost_function,sum(y != yc))
cost_function <- as.data.frame(cost_function)
ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function)) +
geom_line(color="red")+
geom_point()+
xlab("Iterations")+
ylab("Cost")+
ggtitle("Cost Function")
}
PerceptronFunction(x,y,learning.rate = 0.00001)
PlotData <- function(x, y, w, b) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(-b/w[2],-w[1]/w[2])
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PerceptronFunction <- function(x, y, learning.rate = 0.001) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
cost_function <- c(cost_function,sum(y != yc))
# In order to see results well, we want to see the line when it is inside the value interval of our elements to classify. This is when x1 = 0 and x2 < 1, son when x2 < -b/w2, as our line is defined by x2 = -w1*w2 * x1 - b/w2.
if((-b/w[2])<1){PlotData(x,y, w, b)}
}
}
}
cost_function <- c(cost_function,sum(y != yc))
cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
}
PerceptronFunction(x, y, learning.rate = 1)
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
# cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
pf <- PerceptronFunction (x, y, learning.rate = 0.001)
pf
# - $-b/w[2] = 1/2$
# - $-w[1]/w[2]x1 = x1$
-pf$b/pf$w[2]
-pf$w[1]/pf$w[2]
# - $-b/w[2] = 1/2$
# - $-w[1]/w[2]x1 = x1$
-pf$b/pf$w[2]
-pf$w[1]/pf$w[2]
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
# cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
pf <- PerceptronFunction (x, y, learning.rate = 0.001)
# - $-b/w[2] = 1/2$
# - $-w[1]/w[2]x1 = x1$
cat("-b/w[2]: == 1/2?", -pf$b/pf$w[2])
cat("-w[1]/w[2]x1 == x1:?", -pf$b/pf$w[2])
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
# cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
pf <- PerceptronFunction (x, y, learning.rate = 0.001)
# - $-b/w[2] = 1/2$
# - $-w[1]/w[2]x1 = x1$
cat("-b/w[2]: == 1/2?", -pf$b/pf$w[2],"\n")
cat("-w[1]/w[2]x1 == x1:?", -pf$b/pf$w[2])
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
# cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
pf <- PerceptronFunction (x, y, learning.rate = 0.001)
# - $-b/w[2] = 1/2$
# - $-w[1]/w[2]x1 = x1$
cat("-b/w[2] == 1/2?:", -pf$b/pf$w[2],"\n")
cat("-w[1]/w[2]x1 == x1?:", -pf$w[1]/pf$w[2])
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
# cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
# print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
pf <- PerceptronFunction (x, y, learning.rate = 0.001)
# - $-b/w[2] = 1/2$
# - $-w[1]/w[2]x1 = x1$
cat("-b/w[2] == 1/2?:", -pf$b/pf$w[2],"\n")
cat("-w[1]/w[2]x1 == x1?:", -pf$w[1]/pf$w[2])
library(ggplot2)
DistanceFromPlane = function(z, w, b) {
sum(z * w) + b
}
ClassifyLinear = function(x, w, b) {
distances = apply(x, 1, DistanceFromPlane, w, b)
return(ifelse(distances < 0, -1, +1))
}
EuclideanNorm <- function(x) {
return(sqrt(sum(x * x)))
}
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
# very easy
# x2 = x1 + 1/2
set.seed(1234)
x1 <- runif(100,-1,1)
x2 <- runif(100,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
PlotData <- function(x, y) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(0.5,1)
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PlotData(x, y)
the_perceptron <- PerceptronFunction(x,y)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(sum(abs(y - predicted_y)))
# Trying other learning rates.
#############
lr = 0.00001#
#############
start_time <- Sys.time()
the_perceptron <- PerceptronFunction(x,y,learning.rate = lr)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(sum(abs(y - predicted_y)))
end_time <- Sys.time()
# end_time - start_time
# Time difference of 4.836457 secs  with a learning.rate = 0.5
# Time difference of 4.883949 secs  with a learning.rate = 0.89.
# Time difference of 4.891064 secs  with a learning.rate = 0.99999
# Time difference of 4.894685 secs with a learning.rate = 0.00001
# Time difference of 4.941519 secs with a learning.rate = 0.0001
# Time difference of 5.081375 secs with a  learning.rate = 1.
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
cost_function <- c(cost_function,sum(y != yc))
}
}
}
cost_function <- c(cost_function,sum(y != yc))
cost_function <- as.data.frame(cost_function)
ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function)) +
geom_line(color="red")+
geom_point()+
xlab("Iterations")+
ylab("Cost")+
ggtitle("Cost Function")
}
PerceptronFunction(x,y,learning.rate = 0.00001)
PlotData <- function(x, y, w, b) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(-b/w[2],-w[1]/w[2])
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PerceptronFunction <- function(x, y, learning.rate = 0.001) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
cost_function <- c(cost_function,sum(y != yc))
# In order to see results well, we want to see the line when it is inside the value interval of our elements to classify. This is when x1 = 0 and x2 < 1, son when x2 < -b/w2, as our line is defined by x2 = -w1*w2 * x1 - b/w2.
if((-b/w[2])<1){PlotData(x,y, w, b)}
}
}
}
cost_function <- c(cost_function,sum(y != yc))
cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
}
PerceptronFunction(x, y, learning.rate = 1)
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
# cost_function <- c()
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
# cost_function <- c(cost_function,sum(y != yc))
}
}
}
s = EuclideanNorm(w)
# print(s)
# cost_function <- c(cost_function,sum(y != yc))
# cost_function <- as.data.frame(cost_function)
# ggplot(data=cost_function, aes(x=seq(1,length(cost_function)), y= cost_function, group=1)) +
#     geom_line(color="red")+
#     geom_point()+
#     xlab("Iterations")
return(list(w = w/s, b = b/s, steps = iterations))
}
pf <- PerceptronFunction (x, y, learning.rate = 0.001)
# - $-b/w[2] = 1/2$
# - $-w[1]/w[2]x1 = x1$
cat("-b/w[2] == 1/2?:", -pf$b/pf$w[2],"\n")
cat("-w[1]/w[2]x1 == x1?:", -pf$w[1]/pf$w[2])
